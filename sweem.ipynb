{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWEEM Model Implementation\n",
    "\n",
    "This file is used to illustrate the preprocessing, training, and evaluation \n",
    "stages of our model. Comments and more information will be provided per section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Here we load in the data and establish our train-test split. We also set up dataloaders\n",
    "for us to be able to properly use the data within our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_data = pd.read_csv('./Data/Multiple/train.csv')\n",
    "test_data = pd.read_csv('./Data/Multiple/test.csv')\n",
    "\n",
    "## Training Data\n",
    "rna_train = train_data.columns[:2708]                       # 2708 rna\n",
    "scna_train = train_data.columns[2708:5404]                  # 2696 scna\n",
    "mutation_train = train_data.columns[5404:5591]              # 187 mutation\n",
    "methy_train = train_data.columns[5591:7957]                 # 2366 methy\n",
    "target_train = train_data.columns[-3:]                      # 3 target\n",
    "\n",
    "## Testing Data\n",
    "rna_test = test_data.columns[:2708]                         # 2708 rna\n",
    "scna_test = test_data.columns[2708:5404]                    # 2696 scna\n",
    "mutation_test = test_data.columns[5404:5591]                # 187 mutation\n",
    "methy_test = test_data.columns[5591:7957]                   # 2366 methy\n",
    "target_test = test_data.columns[-3:]                        # 3 target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  (380, 15896)\n",
      "Testing Data Shape:  (48, 15896)\n",
      "\n",
      "RNA Labels:         ['SLC4A7_rna', 'EIF5A_rna'] ... ['PRKAR1B_rna', 'NFKBIA_rna']\n",
      "SCNA Labels:        ['ZNF337_rna', 'BECN1_rna'] ... ['TAF9B_rna', 'DUSP3_rna']\n",
      "Mutation Labels:    ['SLC30A5_rna', 'ABCA1_rna'] ... ['MAPK3_scna', 'HSD17B3_scna']\n",
      "Methylation Labels: ['RPS23_scna', 'AKT3_scna'] ... ['PMF1_scna', 'CSN2_scna']\n",
      "Target Labels:      ['SAMPLE_ID', 'OS_MONTHS', 'OS_EVENT']\n"
     ]
    }
   ],
   "source": [
    "### Sanity Checks on Data\n",
    "\n",
    "# Data Shapes; should have same number of features\n",
    "print('Training Data Shape: ', train_data.shape)    # (380, 7961)\n",
    "print('Testing Data Shape: ', test_data.shape)      # (48 , 7961)\n",
    "print()\n",
    "\n",
    "# Check header information\n",
    "print(f\"RNA Labels:         {list(train_data.columns[0:2])} ... {list(train_data.columns[2706:2708])}\")\n",
    "print(f\"SCNA Labels:        {list(train_data.columns[2708:2710])} ... {list(train_data.columns[5402:5404])}\")\n",
    "print(f\"Mutation Labels:    {list(train_data.columns[5404:5406])} ... {list(train_data.columns[5589:5591])}\")\n",
    "print(f\"Methylation Labels: {list(train_data.columns[5591:5593])} ... {list(train_data.columns[7955:7957])}\")\n",
    "print(f\"Target Labels:      {list(train_data.columns[-3:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RIGHT NOW, WE ONLY CARE ABOUT OS_MONTHS, WHICH IS THE SECOND TO LAST COLUMN\n",
    "\n",
    "# Split the data into train and validation sets.\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(\n",
    "    train_data.iloc[:, :-3], train_data.iloc[:, -2], test_size=0.2, random_state=42)\n",
    "\n",
    "# Data in month, event format\n",
    "train_features_alt, val_features_alt, train_labels_alt, val_labels_alt = train_test_split(\n",
    "    train_data.iloc[:, :-3], train_data.iloc[:, -2:], test_size=0.2, random_state=42)\n",
    "\n",
    "test_features, test_labels = test_data.iloc[:, :-3], test_data.iloc[:, -2]\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_dataset = TensorDataset(torch.tensor(train_features.values), torch.tensor(train_labels.values))\n",
    "train_dataset_alt = TensorDataset(torch.tensor(train_features_alt.values), torch.tensor(train_labels_alt.values))\n",
    "val_dataset   = TensorDataset(torch.tensor(val_features.values),   torch.tensor(val_labels.values))\n",
    "test_dataset  = TensorDataset(torch.tensor(test_features.values),  torch.tensor(test_labels.values))\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_dataloader_alt = DataLoader(train_dataset_alt, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "val_dataloader_alt = DataLoader(train_dataset_alt, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Self-Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (model.py, line 9)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\ericj\\miniconda3\\envs\\sweem\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3550\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 1\u001b[1;36m\n\u001b[1;33m    from model import SelfAttentionModel\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\ericj\\browncs\\sweem\\model.py:9\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.self_attention = SelfAttention(Hidden_Nodes)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mTabError\u001b[0m\u001b[1;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "from model import SelfAttentionModel \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = SelfAttentionModel(7961-3, 2000, 2000)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on\", device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 300)\n",
    "        self.linear3 = nn.Linear(300, 250)\n",
    "        self.linear4 = nn.Linear(250, 100)\n",
    "        self.linear5 = nn.Linear(100, 5)\n",
    "        self.linearOut = nn.Linear(5 + 1, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, x2):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear5(out)\n",
    "        out = self.relu(out)\n",
    "        out = torch.cat((out, x2), 1)\n",
    "        out = self.linearOut(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "model = LinearRegression(7961-3, 6000, 1)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on\", device)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Loop\n",
    "from loss import R_set, neg_par_log_likelihood\n",
    "\n",
    "num_epochs = 10\n",
    "epoch_train_losses = []\n",
    "epoch_val_losses   = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss = 0\n",
    "    epoch_val_loss   = 0\n",
    "    print(f\"Epoch {epoch + 1} training:\")\n",
    "    progress_bar = tqdm(range(len(train_dataloader)))\n",
    "\n",
    "    model.train()\n",
    "    ## Training\n",
    "    for (batchX, batchY) in train_dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(batchX.to(device).to(torch.float32))\n",
    "        outputs = outputs.squeeze()\n",
    "\n",
    "        # MSE Backward pass\n",
    "        loss = criterion(outputs, batchY.to(device).to(torch.float32))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    model.eval()\n",
    "    ## Validation\n",
    "    with torch.no_grad():\n",
    "        for (batchX, batchY) in val_dataloader:\n",
    "            outputs = model(batchX.to(device).to(torch.float32))\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, batchY.to(device).to(torch.float32))\n",
    "            epoch_val_loss += loss.item()\n",
    "\n",
    "    # Save and print losses\n",
    "    epoch_train_loss /= len(train_dataloader)\n",
    "    epoch_val_loss /= len(val_dataloader)\n",
    "    epoch_train_losses.append(epoch_train_loss)\n",
    "    epoch_val_losses.append(epoch_val_loss)\n",
    "    print(f\"Epoch {epoch + 1} training loss: {epoch_train_loss}\")\n",
    "    print(f\"Epoch {epoch + 1} validation loss: {epoch_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop with Alt Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Loop\n",
    "from loss import neg_par_log_likelihood, temp_loss\n",
    "num_epochs = 10\n",
    "epoch_train_losses = []\n",
    "epoch_val_losses   = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss = 0\n",
    "    epoch_val_loss   = 0\n",
    "    print(f\"Epoch {epoch + 1} training:\")\n",
    "    progress_bar = tqdm(range(len(train_dataloader_alt)))\n",
    "\n",
    "    model.train()\n",
    "    ## Training\n",
    "    for (batchX, batchY) in train_dataloader_alt:\n",
    "        # Forward pass\n",
    "        #print(model.linear1.weight)\n",
    "        outputs = model(batchX.to(device).to(torch.float32))\n",
    "        #print(batchX)\n",
    "        # print(outputs.shape)\n",
    "        #outputs = outputs.squeeze()\n",
    "\n",
    "        \n",
    "        \n",
    "        # Alt Backward Pass\n",
    "        time, event = batchY[:,0], batchY[:,1]\n",
    "        # print(time.shape) \n",
    "        # print(event.shape)\n",
    "        loss = temp_loss(outputs, time, event)\n",
    "        #loss = neg_par_log_likelihood(outputs, time, event)\n",
    "        print(loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    model.eval()\n",
    "    ## Validation\n",
    "    with torch.no_grad():\n",
    "        for (batchX, batchY) in val_dataloader_alt:\n",
    "            outputs = model(batchX.to(device).to(torch.float32))\n",
    "            #outputs = outputs.squeeze()\n",
    "            time, event = batchY[:,0], batchY[:,1] \n",
    "            loss = temp_loss(outputs, time, event)\n",
    "            #loss = neg_par_log_likelihood(outputs, time, event)\n",
    "            epoch_val_loss += loss.item()\n",
    "\n",
    "    # Save and print losses\n",
    "    epoch_train_loss /= len(train_dataloader_alt)\n",
    "    epoch_val_loss /= len(val_dataloader_alt)\n",
    "    epoch_train_losses.append(epoch_train_loss)\n",
    "    epoch_val_losses.append(epoch_val_loss)\n",
    "    print(f\"Epoch {epoch + 1} training loss: {epoch_train_loss}\")\n",
    "    print(f\"Epoch {epoch + 1} validation loss: {epoch_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intended Output:  tensor([ 837,  800,  785,  403, 1152,  984,  629,  433])\n",
      "Actual Output:  tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### Sanity Checking Outputs\n",
    "\n",
    "# Check the outputs of the model on the test set\n",
    "model.eval()\n",
    "sample_number = 5\n",
    "for test_images, test_labels in test_dataloader: \n",
    "    outputs = model(test_images.to(device).to(torch.float32))\n",
    "    print(\"Intended Output: \", test_labels)\n",
    "    print(\"Actual Output: \", outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci1470",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
