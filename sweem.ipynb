{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWEEM Model Implementation\n",
    "\n",
    "This file is used to illustrate the preprocessing, training, and evaluation \n",
    "stages of our model. Comments and more information will be provided per section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Here we load in the data and establish our train-test split. We also set up dataloaders\n",
    "for us to be able to properly use the data within our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_data = pd.read_csv('./Data/Multiple/train.csv')\n",
    "test_data = pd.read_csv('./Data/Multiple/test.csv')\n",
    "\n",
    "## Training Data\n",
    "rna_train = train_data.columns[:2708]                       # 2708 rna\n",
    "scna_train = train_data.columns[2708:5404]                  # 2696 scna\n",
    "mutation_train = train_data.columns[5404:5591]              # 187 mutation\n",
    "methy_train = train_data.columns[5591:7957]                 # 2366 methy\n",
    "target_train = train_data.columns[-3:]                      # 3 target\n",
    "\n",
    "## Testing Data\n",
    "rna_test = test_data.columns[:2708]                         # 2708 rna\n",
    "scna_test = test_data.columns[2708:5404]                    # 2696 scna\n",
    "mutation_test = test_data.columns[5404:5591]                # 187 mutation\n",
    "methy_test = test_data.columns[5591:7957]                   # 2366 methy\n",
    "target_test = test_data.columns[-3:]                        # 3 target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  (380, 7961)\n",
      "Testing Data Shape:  (48, 7961)\n",
      "\n",
      "RNA Labels:         ['ST3GAL2_rna', 'ELOVL1_rna'] ... ['TK2_rna', 'DHODH_rna']\n",
      "SCNA Labels:        ['ST3GAL2_scna', 'ELOVL1_scna'] ... ['TK2_scna', 'DHODH_scna']\n",
      "Mutation Labels:    ['ABCG4_mutation', 'DAPK1_mutation'] ... ['CDH4_mutation', 'ACSL4_mutation']\n",
      "Methylation Labels: ['ST3GAL2_methy', 'ELOVL1_methy'] ... ['PRKCQ_methy', 'TK2_methy']\n",
      "Target Labels:      ['SAMPLE_ID', 'OS_MONTHS', 'OS_EVENT']\n"
     ]
    }
   ],
   "source": [
    "### Sanity Checks on Data\n",
    "\n",
    "# Data Shapes; should have same number of features\n",
    "print('Training Data Shape: ', train_data.shape)    # (380, 7961)\n",
    "print('Testing Data Shape: ', test_data.shape)      # (48 , 7961)\n",
    "print()\n",
    "\n",
    "# Check header information\n",
    "print(f\"RNA Labels:         {list(train_data.columns[0:2])} ... {list(train_data.columns[2706:2708])}\")\n",
    "print(f\"SCNA Labels:        {list(train_data.columns[2708:2710])} ... {list(train_data.columns[5402:5404])}\")\n",
    "print(f\"Mutation Labels:    {list(train_data.columns[5404:5406])} ... {list(train_data.columns[5589:5591])}\")\n",
    "print(f\"Methylation Labels: {list(train_data.columns[5591:5593])} ... {list(train_data.columns[7955:7957])}\")\n",
    "print(f\"Target Labels:      {list(train_data.columns[-3:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ground truth: OS_MONTHS, OS_EVENT\n",
    "\n",
    "# Split the data into train and validation sets.\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(\n",
    "    train_data.iloc[:, :-3], train_data.iloc[:, -2], test_size=0.2, random_state=42)\n",
    "\n",
    "# Data in month, event format\n",
    "train_features_alt, val_features_alt, train_labels_alt, val_labels_alt = train_test_split(\n",
    "    train_data.iloc[:, :-3], train_data.iloc[:, -2:], test_size=0.2, random_state=42)\n",
    "\n",
    "test_features, test_labels = test_data.iloc[:, :-3], test_data.iloc[:, -2]\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_dataset = TensorDataset(torch.tensor(train_features.values), torch.tensor(train_labels.values))\n",
    "train_dataset_alt = TensorDataset(torch.tensor(train_features_alt.values), torch.tensor(train_labels_alt.values))\n",
    "val_dataset   = TensorDataset(torch.tensor(val_features.values),   torch.tensor(val_labels.values))\n",
    "test_dataset  = TensorDataset(torch.tensor(test_features.values),  torch.tensor(test_labels.values))\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_dataloader_alt = DataLoader(train_dataset_alt, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "val_dataloader_alt = DataLoader(train_dataset_alt, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Self-Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SelfAttentionModel \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = SelfAttentionModel(7961-3, 2000, 2000)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on\", device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 300)\n",
    "        self.linear3 = nn.Linear(300, 250)\n",
    "        self.linear4 = nn.Linear(250, 100)\n",
    "        self.linear5 = nn.Linear(100, 5)\n",
    "        self.linearOut = nn.Linear(5, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, x2):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linearOut(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "model = LinearRegression(7961-3, 6000, 1)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on\", device)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Loop\n",
    "from loss import R_set, neg_par_log_likelihood\n",
    "\n",
    "num_epochs = 10\n",
    "epoch_train_losses = []\n",
    "epoch_val_losses   = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss = 0\n",
    "    epoch_val_loss   = 0\n",
    "    print(f\"Epoch {epoch + 1} training:\")\n",
    "    progress_bar = tqdm(range(len(train_dataloader)))\n",
    "\n",
    "    model.train()\n",
    "    ## Training\n",
    "    for (batchX, batchY) in train_dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(batchX.to(device).to(torch.float32))\n",
    "        outputs = outputs.squeeze()\n",
    "\n",
    "        # MSE Backward pass\n",
    "        loss = criterion(outputs, batchY.to(device).to(torch.float32))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    model.eval()\n",
    "    ## Validation\n",
    "    with torch.no_grad():\n",
    "        for (batchX, batchY) in val_dataloader:\n",
    "            outputs = model(batchX.to(device).to(torch.float32))\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, batchY.to(device).to(torch.float32))\n",
    "            epoch_val_loss += loss.item()\n",
    "\n",
    "    # Save and print losses\n",
    "    epoch_train_loss /= len(train_dataloader)\n",
    "    epoch_val_loss /= len(val_dataloader)\n",
    "    epoch_train_losses.append(epoch_train_loss)\n",
    "    epoch_val_losses.append(epoch_val_loss)\n",
    "    print(f\"Epoch {epoch + 1} training loss: {epoch_train_loss}\")\n",
    "    print(f\"Epoch {epoch + 1} validation loss: {epoch_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop with Alt Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Loop\n",
    "from loss import neg_par_log_likelihood, temp_loss\n",
    "num_epochs = 10\n",
    "epoch_train_losses = []\n",
    "epoch_val_losses   = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss = 0\n",
    "    epoch_val_loss   = 0\n",
    "    print(f\"Epoch {epoch + 1} training:\")\n",
    "    progress_bar = tqdm(range(len(train_dataloader_alt)))\n",
    "\n",
    "    model.train()\n",
    "    ## Training\n",
    "    for (batchX, batchY) in train_dataloader_alt:\n",
    "        # Forward pass\n",
    "        outputs = model(batchX.to(device).to(torch.float32))\n",
    "        #print(outputs)\n",
    "        #outputs = outputs.squeeze()\n",
    "        \n",
    "        # Alt Backward Pass\n",
    "        time, event = batchY[:,0], batchY[:,1]\n",
    "        loss = temp_loss(outputs, time, event)\n",
    "        #loss = neg_par_log_likelihood(outputs, time, event)\n",
    "        #print(loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    model.eval()\n",
    "    ## Validation\n",
    "    with torch.no_grad():\n",
    "        for (batchX, batchY) in val_dataloader_alt:\n",
    "            outputs = model(batchX.to(device).to(torch.float32))\n",
    "            #outputs = outputs.squeeze()\n",
    "            time, event = batchY[:,0], batchY[:,1] \n",
    "            loss = temp_loss(outputs, time, event)\n",
    "            #loss = neg_par_log_likelihood(outputs, time, event)\n",
    "            epoch_val_loss += loss.item()\n",
    "\n",
    "    # Save and print losses\n",
    "    epoch_train_loss /= len(train_dataloader_alt)\n",
    "    epoch_val_loss /= len(val_dataloader_alt)\n",
    "    epoch_train_losses.append(epoch_train_loss)\n",
    "    epoch_val_losses.append(epoch_val_loss)\n",
    "    print(f\"Epoch {epoch + 1} training loss: {epoch_train_loss}\")\n",
    "    print(f\"Epoch {epoch + 1} validation loss: {epoch_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intended Output:  tensor([0, 0, 0, 0, 1, 1, 0, 0])\n",
      "Actual Output:  tensor([[0.3996],\n",
      "        [0.3996],\n",
      "        [0.3996],\n",
      "        [0.3996],\n",
      "        [0.3996],\n",
      "        [0.3996],\n",
      "        [0.3996],\n",
      "        [0.3996]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "### Sanity Checking Outputs\n",
    "\n",
    "# Check the outputs of the model on the test set\n",
    "model.eval()\n",
    "sample_number = 5\n",
    "for test_images, test_labels in test_dataloader: \n",
    "    outputs = model(test_images.to(device).to(torch.float32))\n",
    "    print(\"Intended Output: \", test_labels)\n",
    "    print(\"Actual Output: \", outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci1470",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
