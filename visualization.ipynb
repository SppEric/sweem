{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations\n",
    "\n",
    "This notebook assists with loading visualization-related items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import checkpoint\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from model import SWEEM\n",
    "from visualization import visualizePathwayModules, visualizeModelAttention\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Data/OmicsData/data.csv')\n",
    "\n",
    "# Separate to make sure that there's an even distribution of 1s and 0s in train and test\n",
    "data_ones = data[data.iloc[:, -1] == 1]\n",
    "data_zeros = data[data.iloc[:, -1] == 0]\n",
    "\n",
    "# Split the data into train and validation sets.\n",
    "train_data_ones, test_data_ones, train_labels_ones, test_labels_ones = train_test_split(\n",
    "    data_ones.iloc[:, 1:-2], data_ones.iloc[:, -2:], test_size=0.2, random_state=42)\n",
    "train_data_zeros, test_data_zeros, train_labels_zeros, test_labels_zeros = train_test_split(\n",
    "    data_zeros.iloc[:, 1:-2], data_zeros.iloc[:, -2:], test_size=0.2, random_state=42)\n",
    "\n",
    "# Concatenate in the end to make train and test\n",
    "train_data = pd.concat((train_data_ones, train_data_zeros))\n",
    "train_labels = pd.concat((train_labels_ones, train_labels_zeros))\n",
    "test_data = pd.concat((test_data_ones, test_data_zeros)) \n",
    "test_labels = pd.concat((test_labels_ones, test_labels_zeros))\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_dataset = TensorDataset(torch.tensor(train_data.values, dtype=torch.float32), torch.tensor(train_labels.values, dtype=torch.float32))\n",
    "test_dataset  = TensorDataset(torch.tensor(test_data.values, dtype=torch.float32), torch.tensor(test_labels.values, dtype=torch.float32))\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "settings = {\n",
    "    \"model\": {\n",
    "        \"rna_dim\": 5540,\n",
    "        \"scna_dim\": 5507,\n",
    "        \"methy_dim\": 4846,\n",
    "        \"use_rna\": False,\n",
    "        \"use_scna\": False,\n",
    "        \"use_methy\": True,\n",
    "        \"hidden_dim\": 32,\n",
    "        \"self_att\": True,\n",
    "        \"cross_att\": False,\n",
    "        \"device\": device\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"lr\": 0.00001,\n",
    "        \"l2\": 1e-3,\n",
    "        \"epochs\":1001,\n",
    "        \"epoch_mod\": 25\n",
    "    }\n",
    "}\n",
    "\n",
    "### Model trained on methylation data. \n",
    "model, optimizer, epoch_train_losses, epoch_val_losses, settings = checkpoint.load(\"./sweem.model\", SWEEM, optim.Adam)\n",
    "model = SWEEM(**settings[\"model\"])\n",
    "model.load_state_dict(torch.load(\"./sweem_inference.model\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizePathwayModules(rna=True, scna=True, methy=True)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_gradients(model, event, kwarg_dict, baseline=None, num_steps=50):\n",
    "    input_data = kwarg_dict['methy']\n",
    "\n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(input_data)\n",
    "\n",
    "    scaled_inputs = [baseline + (float(i) / num_steps) * (input_data - baseline) for i in range(num_steps + 1)]\n",
    "\n",
    "    total_gradients = torch.zeros_like(input_data)\n",
    "\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    for scaled_input in scaled_inputs:\n",
    "        scaled_input.requires_grad_()\n",
    "        print(scaled_input.shape)\n",
    "        print(kwarg_dict['rna'].shape)\n",
    "        # Compute the model outputs\n",
    "        outputs = model(event=event, rna=kwarg_dict['rna'], scna=kwarg_dict['scna'], methy=scaled_input)\n",
    "        # Compute the loss between the model outputs and the target\n",
    "        loss = criterion(outputs, event).sum()\n",
    "        kwarg_dict['methy'] = scaled_input\n",
    "        \n",
    "        input = (event, kwarg_dict['rna'], kwarg_dict['scna'], kwarg_dict['methy'])\n",
    "        grads = torch.autograd.grad(loss, input)[0]\n",
    "        total_gradients += grads.detach()  # Detach to prevent memory leak\n",
    "\n",
    "    # Calculate integrated gradients after the loop\n",
    "    integrated_grad = (input_data - baseline) * total_gradients / num_steps\n",
    "    return integrated_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated Gradients Interptability.\n",
    "model.device = device\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (batchX, batchY) in enumerate(test_dataloader):\n",
    "        batchX = batchX.to(device)\n",
    "        rna = batchX[:, :5540].to(device)\n",
    "        scna = batchX[:, 5540:11047].to(device)\n",
    "        methy = batchX[:, 11047:].to(device)\n",
    "        time = batchY[:, 0].reshape(-1, 1).to(device)\n",
    "        event = batchY[:, 1].reshape(-1, 1).to(device)\n",
    "        outputs = model(event=event, rna=rna, scna=scna, methy=methy)\n",
    "\n",
    "        kwarg_dict = {'rna':rna, 'scna':scna, 'methy':methy}\n",
    "        baseline = torch.zeros_like(methy)\n",
    "        integrated_grad = integrated_gradients(model, event, kwarg_dict, baseline=baseline)\n",
    "\n",
    "        print(integrated_gradients)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempted Model Attention Perturbation-based Interpretation.\n",
    "## The captum attr library does not seem to support our input format of **kwargs,\n",
    "## so we will have to rework the model to take in a single input. For now, we focus\n",
    "## on other attention-based interpretability methods that we can use.\n",
    "from captum.attr import LayerGradientXActivation\n",
    "model.device = device\n",
    "\n",
    "layer_grad_x_activation = LayerGradientXActivation(model, model.methyl_att)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (batchX, batchY) in enumerate(test_dataloader):\n",
    "        batchX = batchX.to(device)\n",
    "        rna = batchX[:, :5540].to(device)\n",
    "        scna = batchX[:, 5540:11047].to(device)\n",
    "        methy = batchX[:, 11047:].to(device)\n",
    "        time = batchY[:, 0].reshape(-1, 1).to(device)\n",
    "        event = batchY[:, 1].reshape(-1, 1).to(device)\n",
    "        outputs = model(event=event, rna=rna, scna=scna, methy=methy)\n",
    "\n",
    "        attributions, delta = layer_grad_x_activation.attribute({'methy':methy})\n",
    "        # attributions = attributions.squeeze().cpu().detach().numpy()\n",
    "\n",
    "        # fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        # axs[0].imshow(methy[0].cpu().detach().numpy(), cmap='viridis', aspect='auto')\n",
    "        # axs[0].set_title('Original Methy Input')\n",
    "\n",
    "        # axs[1].imshow((methy + delta)[0].cpu().detach().numpy(), cmap='viridis', aspect='auto')\n",
    "        # axs[1].set_title('Perturbed Methy Input')\n",
    "\n",
    "        # axs[2].imshow(attributions[0], cmap='seismic', aspect='auto')\n",
    "        # axs[2].set_title('Attributions')\n",
    "\n",
    "        # plt.show()\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack@brown",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
